{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\tSuccessfully loaded train_identity!\n",
      "\tSuccessfully loaded train_transaction!\n",
      "\tSuccessfully loaded test_identity!\n",
      "\tSuccessfully loaded test_transaction!\n",
      "Data was successfully loaded!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "\n",
    "train_identity = pd.read_csv('train_identity.csv', index_col='TransactionID')\n",
    "print('\\tSuccessfully loaded train_identity!')\n",
    "\n",
    "train_transaction = pd.read_csv('train_transaction.csv', index_col='TransactionID')\n",
    "print('\\tSuccessfully loaded train_transaction!')\n",
    "\n",
    "test_identity = pd.read_csv('test_identity.csv', index_col='TransactionID')\n",
    "print('\\tSuccessfully loaded test_identity!')\n",
    "\n",
    "test_transaction = pd.read_csv('test_transaction.csv', index_col='TransactionID')\n",
    "print('\\tSuccessfully loaded test_transaction!')\n",
    "\n",
    "print('Data was successfully loaded!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSuccessfully loaded sample_submission!\n"
     ]
    }
   ],
   "source": [
    "sub = pd.read_csv('sample_submission.csv')\n",
    "print('\\tSuccessfully loaded sample_submission!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefull methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage after optimization is: {end_mem:.2f} MB')\n",
    "    print(f'Decreased by {100 * (start_mem - end_mem) / start_mem:.1f}%')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_split(dataframe):\n",
    "    dataframe['device_name'] = dataframe['DeviceInfo'].str.split('/', expand=True)[0]\n",
    "    dataframe['device_version'] = dataframe['DeviceInfo'].str.split('/', expand=True)[1]\n",
    "\n",
    "    dataframe['OS_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[0]\n",
    "    dataframe['version_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[1]\n",
    "\n",
    "    dataframe['browser_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[0]\n",
    "    dataframe['version_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[1]\n",
    "\n",
    "    dataframe['screen_width'] = dataframe['id_33'].str.split('x', expand=True)[0]\n",
    "    dataframe['screen_height'] = dataframe['id_33'].str.split('x', expand=True)[1]\n",
    "\n",
    "    # dataframe['id_34'] = dataframe['id_34'].str.split(':', expand=True)[1]\n",
    "    dataframe['id_23'] = dataframe['id_23'].str.split(':', expand=True)[1]\n",
    "\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n",
    "\n",
    "    dataframe.loc[dataframe.device_name.isin(\n",
    "        dataframe.device_name.value_counts()[dataframe.device_name.value_counts() < 200].index), \n",
    "                  'device_name'] = \"Others\"\n",
    "    dataframe['had_id'] = 1\n",
    "    gc.collect()\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_identity = id_split(train_identity)\n",
    "test_identity = id_split(test_identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging data...\n",
      "Data was successfully merged!\n",
      "\n",
      "Train dataset has 590540 rows and 442 columns.\n",
      "Test dataset has 506691 rows and 441 columns.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Merging data...')\n",
    "train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n",
    "test = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n",
    "\n",
    "print('Data was successfully merged!\\n')\n",
    "\n",
    "del train_identity, train_transaction, test_identity, test_transaction\n",
    "\n",
    "print(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\n",
    "print(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_features = ['TransactionAmt', 'ProductCD', \n",
    "                   'card1', 'card2', 'card3', 'card4', 'card5', 'card6', \n",
    "                   'addr1', 'addr2', 'dist1',\n",
    "                   'P_emaildomain', 'R_emaildomain', \n",
    "                   'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13',\n",
    "                   'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D8', 'D9', \n",
    "                   'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M2', 'M3',\n",
    "                   'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', \n",
    "                   'V10', 'V11', 'V12', 'V13', 'V17',\n",
    "                   'V19', 'V20', 'V29', 'V30', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V40', 'V44', \n",
    "                   'V45', 'V46', 'V47', 'V48',\n",
    "                   'V49', 'V51', 'V52', 'V53', 'V54', 'V56', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', \n",
    "                   'V64', 'V69', 'V70', 'V71',\n",
    "                   'V72', 'V73', 'V74', 'V75', 'V76', 'V78', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', \n",
    "                   'V87', 'V90', 'V91', 'V92',\n",
    "                   'V93', 'V94', 'V95', 'V96', 'V97', 'V99', 'V100', 'V126', 'V127', 'V128', 'V130', \n",
    "                   'V131', 'V138', 'V139', 'V140',\n",
    "                   'V143', 'V145', 'V146', 'V147', 'V149', 'V150', 'V151', 'V152', 'V154', 'V156', \n",
    "                   'V158', 'V159', 'V160', 'V161',\n",
    "                   'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V169', 'V170', 'V171', 'V172', \n",
    "                   'V173', 'V175', 'V176', 'V177',\n",
    "                   'V178', 'V180', 'V182', 'V184', 'V187', 'V188', 'V189', 'V195', 'V197', 'V200', \n",
    "                   'V201', 'V202', 'V203', 'V204',\n",
    "                   'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V212', 'V213', 'V214', 'V215', \n",
    "                   'V216', 'V217', 'V219', 'V220',\n",
    "                   'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V231', \n",
    "                   'V233', 'V234', 'V238', 'V239',\n",
    "                   'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V249', 'V251', 'V253', 'V256', \n",
    "                   'V257', 'V258', 'V259', 'V261',\n",
    "                   'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V270', 'V271', 'V272', \n",
    "                   'V273', 'V274', 'V275', 'V276',\n",
    "                   'V277', 'V278', 'V279', 'V280', 'V282', 'V283', 'V285', 'V287', 'V288', 'V289', \n",
    "                   'V291', 'V292', 'V294', 'V303',\n",
    "                   'V304', 'V306', 'V307', 'V308', 'V310', 'V312', 'V313', 'V314', 'V315', 'V317', \n",
    "                   'V322', 'V323', 'V324', 'V326',\n",
    "                   'V329', 'V331', 'V332', 'V333', 'V335', 'V336', 'V338', 'id_01', 'id_02', 'id_03', 'id_05', \n",
    "                   'id_06', 'id_09',\n",
    "                   'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_17', 'id_19', 'id_20', 'id_30', \n",
    "                   'id_31', 'id_32', 'id_33',\n",
    "                   'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'device_name', \n",
    "                   'device_version', 'OS_id_30', 'version_id_30',\n",
    "                   'browser_id_31', 'version_id_31', 'screen_width', 'screen_height', 'had_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [col for col in train.columns if col not in useful_features]\n",
    "cols_to_drop.remove('isFraud')\n",
    "cols_to_drop.remove('TransactionDT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(cols_to_drop, axis=1)\n",
    "test = test.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_a = ['TransactionAmt', 'id_02', 'D15']\n",
    "columns_b = ['card1', 'card4', 'addr1']\n",
    "\n",
    "for col_a in columns_a:\n",
    "    for col_b in columns_b:\n",
    "        for df in [train, test]:\n",
    "            df[f'{col_a}_to_mean_{col_b}'] = df[col_a] / df.groupby([col_b])[col_a].transform('mean')\n",
    "            df[f'{col_a}_to_std_{col_b}'] = df[col_a] / df.groupby([col_b])[col_a].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New feature - log of transaction amount.\n",
    "train['TransactionAmt_Log'] = np.log(train['TransactionAmt'])\n",
    "test['TransactionAmt_Log'] = np.log(test['TransactionAmt'])\n",
    "\n",
    "# New feature - decimal part of the transaction amount.\n",
    "train['TransactionAmt_decimal'] = ((train['TransactionAmt'] - \n",
    "                                    train['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "test['TransactionAmt_decimal'] = ((test['TransactionAmt'] - \n",
    "                                   test['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "\n",
    "# New feature - day of week in which a transaction happened.\n",
    "train['Transaction_day_of_week'] = np.floor((train['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "test['Transaction_day_of_week'] = np.floor((test['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "\n",
    "# New feature - hour of the day in which a transaction happened.\n",
    "train['Transaction_hour'] = np.floor(train['TransactionDT'] / 3600) % 24\n",
    "test['Transaction_hour'] = np.floor(test['TransactionDT'] / 3600) % 24\n",
    "\n",
    "# Some arbitrary features interaction\n",
    "for feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n",
    "                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n",
    "\n",
    "    f1, f2 = feature.split('__')\n",
    "    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n",
    "    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n",
    "    train[feature] = le.transform(list(train[feature].astype(str).values))\n",
    "    test[feature] = le.transform(list(test[feature].astype(str).values))\n",
    "\n",
    "# Encoding - count encoding for both train and test\n",
    "for feature in ['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'id_36']:\n",
    "    train[feature + '_count_full'] = train[feature].map(pd.concat([train[feature], test[feature]], \n",
    "                                                                  ignore_index=True).value_counts(dropna=False))\n",
    "    test[feature + '_count_full'] = test[feature].map(pd.concat([train[feature], test[feature]], \n",
    "                                                                ignore_index=True).value_counts(dropna=False))\n",
    "\n",
    "# Encoding - count encoding separately for train and test\n",
    "for feature in ['id_01', 'id_31', 'id_33', 'id_36']:\n",
    "    train[feature + '_count_dist'] = train[feature].map(train[feature].value_counts(dropna=False))\n",
    "    test[feature + '_count_dist'] = test[feature].map(test[feature].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriching with email characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', \n",
    "          'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', \n",
    "          'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', \n",
    "          'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': \n",
    "          'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': \n",
    "          'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': \n",
    "          'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', \n",
    "          'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', \n",
    "          'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', \n",
    "          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', \n",
    "          'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', \n",
    "          'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n",
    "          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': \n",
    "          'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', \n",
    "          'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', \n",
    "          'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "us_emails = ['gmail', 'net', 'edu']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email domain Alexa ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import urllib.request\n",
    "x_path_rank = '/html/body/div/div/table/tbody/tr/td[1]/div[1]/a'\n",
    "x_path_traffic_rank = '/html/body/div/div/table/tbody/tr/td[2]/div[1]/a'\n",
    "x_path_sites = '/html/body/div/div/table/tbody/tr/td[3]/div[1]/a'\n",
    "\n",
    "alexa_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_domain(domain):\n",
    "    if domain in ['', None, np.nan]:\n",
    "        return (None, None, None)\n",
    "    \n",
    "    if domain == 'gmail':\n",
    "        domain = 'gmail.com'\n",
    "    \n",
    "    if domain in alexa_cache:\n",
    "        return alexa_cache[domain]\n",
    "    \n",
    "    with urllib.request.urlopen(f\"https://www.alexa.com/minisiteinfo/{domain}\") as url:\n",
    "        print(f'Querying Alexa for ranking of {domain}.')\n",
    "        \n",
    "        s = url.read()\n",
    "        tree = html.fromstring(s)\n",
    "        try:\n",
    "            rank = int(tree.xpath(x_path_rank)[0].text_content().replace(',', ''))\n",
    "        except IndexError:\n",
    "            print(f'No rank for domain {domain}...')\n",
    "            rank = None\n",
    "        except ValueError:\n",
    "            rank = 0\n",
    "            \n",
    "        try:\n",
    "            traffic = int(tree.xpath(x_path_traffic_rank)[0].text_content().replace(',', ''))\n",
    "        except IndexError:\n",
    "            print(f'No rank for domain {domain}...')\n",
    "            traffic = None\n",
    "        except ValueError:\n",
    "            traffic = 0\n",
    "            \n",
    "        try:\n",
    "            sites = int(tree.xpath(x_path_sites)[0].text_content().replace(',', ''))\n",
    "        except IndexError:\n",
    "            print(f'No rank for domain {domain}...')\n",
    "            sites = None       \n",
    "        except ValueError:\n",
    "            sites = 0\n",
    "        \n",
    "        alexa_cache[domain] = (rank, traffic, sites)\n",
    "    \n",
    "        return rank, traffic, sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying Alexa for ranking of servicios-ta.com.\n",
      "No rank for domain servicios-ta.com...\n",
      "No rank for domain servicios-ta.com...\n",
      "Querying Alexa for ranking of earthlink.net.\n",
      "Querying Alexa for ranking of hotmail.es.\n",
      "No rank for domain hotmail.es...\n",
      "Querying Alexa for ranking of cfl.rr.com.\n",
      "Querying Alexa for ranking of roadrunner.com.\n",
      "Querying Alexa for ranking of netzero.net.\n",
      "Querying Alexa for ranking of gmx.de.\n",
      "No rank for domain gmx.de...\n",
      "Querying Alexa for ranking of suddenlink.net.\n",
      "Querying Alexa for ranking of frontiernet.net.\n",
      "Querying Alexa for ranking of windstream.net.\n",
      "Querying Alexa for ranking of frontier.com.\n",
      "Querying Alexa for ranking of outlook.es.\n",
      "No rank for domain outlook.es...\n",
      "Querying Alexa for ranking of mac.com.\n",
      "Querying Alexa for ranking of netzero.com.\n",
      "No rank for domain netzero.com...\n",
      "Querying Alexa for ranking of aim.com.\n",
      "Querying Alexa for ranking of web.de.\n",
      "Querying Alexa for ranking of twc.com.\n",
      "No rank for domain twc.com...\n",
      "Querying Alexa for ranking of cableone.net.\n",
      "Querying Alexa for ranking of yahoo.fr.\n",
      "No rank for domain yahoo.fr...\n",
      "Querying Alexa for ranking of yahoo.de.\n",
      "No rank for domain yahoo.de...\n",
      "Querying Alexa for ranking of yahoo.es.\n",
      "No rank for domain yahoo.es...\n",
      "Querying Alexa for ranking of sc.rr.com.\n",
      "Querying Alexa for ranking of ptd.net.\n",
      "Querying Alexa for ranking of live.fr.\n",
      "No rank for domain live.fr...\n",
      "Querying Alexa for ranking of yahoo.co.uk.\n",
      "No rank for domain yahoo.co.uk...\n",
      "Querying Alexa for ranking of hotmail.fr.\n",
      "No rank for domain hotmail.fr...\n",
      "Querying Alexa for ranking of hotmail.de.\n",
      "No rank for domain hotmail.de...\n",
      "Querying Alexa for ranking of hotmail.co.uk.\n",
      "No rank for domain hotmail.co.uk...\n",
      "Querying Alexa for ranking of protonmail.com.\n",
      "Querying Alexa for ranking of yahoo.co.jp.\n",
      "Querying Alexa for ranking of scranton.edu.\n"
     ]
    }
   ],
   "source": [
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    train[c + '_rank'] = train[c].apply(lambda x: rank_domain(x)[0])\n",
    "    test[c + '_rank'] = test[c].apply(lambda x: rank_domain(x)[0])\n",
    "    \n",
    "    train[c + '_traffic_rank'] = train[c].apply(lambda x: rank_domain(x)[1])\n",
    "    test[c + '_traffic_rank'] = test[c].apply(lambda x: rank_domain(x)[1])\n",
    "    \n",
    "    train[c + '_sites'] = train[c].apply(lambda x: rank_domain(x)[2])\n",
    "    test[c + '_sites'] = test[c].apply(lambda x: rank_domain(x)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other enriching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499\n",
    "\n",
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    train[c + '_bin'] = train[c].map(emails)\n",
    "    test[c + '_bin'] = test[c].map(emails)\n",
    "    \n",
    "    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n",
    "    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    if train[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n",
    "        train[col] = le.transform(list(train[col].astype(str).values))\n",
    "        test[col] = le.transform(list(test[col].astype(str).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1569.88 MB\n",
      "Memory usage after optimization is: 464.91 MB\n",
      "Decreased by 70.4%\n",
      "Memory usage of dataframe is 1345.95 MB\n",
      "Memory usage after optimization is: 411.89 MB\n",
      "Decreased by 69.4%\n"
     ]
    }
   ],
   "source": [
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6459"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT'], axis=1)\n",
    "y = train.sort_values('TransactionDT')['isFraud']\n",
    "\n",
    "X_test = test.drop(['TransactionDT'], axis=1)\n",
    "\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TransactionAmt': 59.0,\n",
       " 'ProductCD': 4.0,\n",
       " 'card1': 4663.0,\n",
       " 'card2': 490.0,\n",
       " 'card3': 150.0,\n",
       " 'card4': 4.0,\n",
       " 'card5': 166.0,\n",
       " 'card6': 2.0,\n",
       " 'addr1': 330.0,\n",
       " 'addr2': 87.0,\n",
       " 'dist1': 287.0,\n",
       " 'P_emaildomain': 36.0,\n",
       " 'R_emaildomain': 32.0,\n",
       " 'C1': 1.0,\n",
       " 'C2': 1.0,\n",
       " 'C4': 0.0,\n",
       " 'C5': 0.0,\n",
       " 'C6': 1.0,\n",
       " 'C7': 0.0,\n",
       " 'C8': 0.0,\n",
       " 'C9': 1.0,\n",
       " 'C10': 0.0,\n",
       " 'C11': 1.0,\n",
       " 'C12': 0.0,\n",
       " 'C13': 1.0,\n",
       " 'C14': 1.0,\n",
       " 'D1': 0.0,\n",
       " 'D2': nan,\n",
       " 'D3': nan,\n",
       " 'D4': 0.0,\n",
       " 'D5': nan,\n",
       " 'D6': nan,\n",
       " 'D8': nan,\n",
       " 'D9': nan,\n",
       " 'D10': 0.0,\n",
       " 'D11': 315.0,\n",
       " 'D12': nan,\n",
       " 'D13': nan,\n",
       " 'D14': nan,\n",
       " 'D15': 315.0,\n",
       " 'M2': 1.0,\n",
       " 'M3': 1.0,\n",
       " 'M4': 0.0,\n",
       " 'M5': 0.0,\n",
       " 'M6': 0.0,\n",
       " 'M7': 0.0,\n",
       " 'M8': 0.0,\n",
       " 'M9': 0.0,\n",
       " 'V3': 1.0,\n",
       " 'V4': 1.0,\n",
       " 'V5': 1.0,\n",
       " 'V6': 1.0,\n",
       " 'V7': 1.0,\n",
       " 'V8': 1.0,\n",
       " 'V9': 1.0,\n",
       " 'V10': 0.0,\n",
       " 'V11': 0.0,\n",
       " 'V12': 1.0,\n",
       " 'V13': 1.0,\n",
       " 'V17': 0.0,\n",
       " 'V19': 1.0,\n",
       " 'V20': 1.0,\n",
       " 'V29': 0.0,\n",
       " 'V30': 0.0,\n",
       " 'V33': 0.0,\n",
       " 'V34': 0.0,\n",
       " 'V35': 1.0,\n",
       " 'V36': 1.0,\n",
       " 'V37': 1.0,\n",
       " 'V38': 1.0,\n",
       " 'V40': 0.0,\n",
       " 'V44': 1.0,\n",
       " 'V45': 1.0,\n",
       " 'V46': 1.0,\n",
       " 'V47': 1.0,\n",
       " 'V48': 0.0,\n",
       " 'V49': 0.0,\n",
       " 'V51': 0.0,\n",
       " 'V52': 0.0,\n",
       " 'V53': 1.0,\n",
       " 'V54': 1.0,\n",
       " 'V56': 1.0,\n",
       " 'V58': 0.0,\n",
       " 'V59': 0.0,\n",
       " 'V60': 0.0,\n",
       " 'V61': 1.0,\n",
       " 'V62': 1.0,\n",
       " 'V63': 0.0,\n",
       " 'V64': 0.0,\n",
       " 'V69': 0.0,\n",
       " 'V70': 0.0,\n",
       " 'V71': 0.0,\n",
       " 'V72': 0.0,\n",
       " 'V73': 0.0,\n",
       " 'V74': 0.0,\n",
       " 'V75': 1.0,\n",
       " 'V76': 1.0,\n",
       " 'V78': 1.0,\n",
       " 'V80': 0.0,\n",
       " 'V81': 0.0,\n",
       " 'V82': 1.0,\n",
       " 'V83': 1.0,\n",
       " 'V84': 0.0,\n",
       " 'V85': 0.0,\n",
       " 'V87': 1.0,\n",
       " 'V90': 0.0,\n",
       " 'V91': 0.0,\n",
       " 'V92': 0.0,\n",
       " 'V93': 0.0,\n",
       " 'V94': 0.0,\n",
       " 'V95': 0.0,\n",
       " 'V96': 0.0,\n",
       " 'V97': 0.0,\n",
       " 'V99': 0.0,\n",
       " 'V100': 0.0,\n",
       " 'V126': 0.0,\n",
       " 'V127': 0.0,\n",
       " 'V128': 0.0,\n",
       " 'V130': 0.0,\n",
       " 'V131': 0.0,\n",
       " 'V138': nan,\n",
       " 'V139': nan,\n",
       " 'V140': nan,\n",
       " 'V143': nan,\n",
       " 'V145': nan,\n",
       " 'V146': nan,\n",
       " 'V147': nan,\n",
       " 'V149': nan,\n",
       " 'V150': nan,\n",
       " 'V151': nan,\n",
       " 'V152': nan,\n",
       " 'V154': nan,\n",
       " 'V156': nan,\n",
       " 'V158': nan,\n",
       " 'V159': nan,\n",
       " 'V160': nan,\n",
       " 'V161': nan,\n",
       " 'V162': nan,\n",
       " 'V163': nan,\n",
       " 'V164': nan,\n",
       " 'V165': nan,\n",
       " 'V166': nan,\n",
       " 'V167': nan,\n",
       " 'V169': nan,\n",
       " 'V170': nan,\n",
       " 'V171': nan,\n",
       " 'V172': nan,\n",
       " 'V173': nan,\n",
       " 'V175': nan,\n",
       " 'V176': nan,\n",
       " 'V177': nan,\n",
       " 'V178': nan,\n",
       " 'V180': nan,\n",
       " 'V182': nan,\n",
       " 'V184': nan,\n",
       " 'V187': nan,\n",
       " 'V188': nan,\n",
       " 'V189': nan,\n",
       " 'V195': nan,\n",
       " 'V197': nan,\n",
       " 'V200': nan,\n",
       " 'V201': nan,\n",
       " 'V202': nan,\n",
       " 'V203': nan,\n",
       " 'V204': nan,\n",
       " 'V205': nan,\n",
       " 'V206': nan,\n",
       " 'V207': nan,\n",
       " 'V208': nan,\n",
       " 'V209': nan,\n",
       " 'V210': nan,\n",
       " 'V212': nan,\n",
       " 'V213': nan,\n",
       " 'V214': nan,\n",
       " 'V215': nan,\n",
       " 'V216': nan,\n",
       " 'V217': nan,\n",
       " 'V219': nan,\n",
       " 'V220': nan,\n",
       " 'V221': nan,\n",
       " 'V222': nan,\n",
       " 'V223': nan,\n",
       " 'V224': nan,\n",
       " 'V225': nan,\n",
       " 'V226': nan,\n",
       " 'V227': nan,\n",
       " 'V228': nan,\n",
       " 'V229': nan,\n",
       " 'V231': nan,\n",
       " 'V233': nan,\n",
       " 'V234': nan,\n",
       " 'V238': nan,\n",
       " 'V239': nan,\n",
       " 'V242': nan,\n",
       " 'V243': nan,\n",
       " 'V244': nan,\n",
       " 'V245': nan,\n",
       " 'V246': nan,\n",
       " 'V247': nan,\n",
       " 'V249': nan,\n",
       " 'V251': nan,\n",
       " 'V253': nan,\n",
       " 'V256': nan,\n",
       " 'V257': nan,\n",
       " 'V258': nan,\n",
       " 'V259': nan,\n",
       " 'V261': nan,\n",
       " 'V262': nan,\n",
       " 'V263': nan,\n",
       " 'V264': nan,\n",
       " 'V265': nan,\n",
       " 'V266': nan,\n",
       " 'V267': nan,\n",
       " 'V268': nan,\n",
       " 'V270': nan,\n",
       " 'V271': nan,\n",
       " 'V272': nan,\n",
       " 'V273': nan,\n",
       " 'V274': nan,\n",
       " 'V275': nan,\n",
       " 'V276': nan,\n",
       " 'V277': nan,\n",
       " 'V278': nan,\n",
       " 'V279': 0.0,\n",
       " 'V280': 0.0,\n",
       " 'V282': 1.0,\n",
       " 'V283': 1.0,\n",
       " 'V285': 0.0,\n",
       " 'V287': 0.0,\n",
       " 'V288': 0.0,\n",
       " 'V289': 0.0,\n",
       " 'V291': 1.0,\n",
       " 'V292': 1.0,\n",
       " 'V294': 0.0,\n",
       " 'V303': 0.0,\n",
       " 'V304': 0.0,\n",
       " 'V306': 0.0,\n",
       " 'V307': 0.0,\n",
       " 'V308': 0.0,\n",
       " 'V310': 0.0,\n",
       " 'V312': 0.0,\n",
       " 'V313': 0.0,\n",
       " 'V314': 0.0,\n",
       " 'V315': 0.0,\n",
       " 'V317': 0.0,\n",
       " 'V322': nan,\n",
       " 'V323': nan,\n",
       " 'V324': nan,\n",
       " 'V326': nan,\n",
       " 'V329': nan,\n",
       " 'V331': nan,\n",
       " 'V332': nan,\n",
       " 'V333': nan,\n",
       " 'V335': nan,\n",
       " 'V336': nan,\n",
       " 'V338': nan,\n",
       " 'id_01': nan,\n",
       " 'id_02': nan,\n",
       " 'id_03': nan,\n",
       " 'id_05': nan,\n",
       " 'id_06': nan,\n",
       " 'id_09': nan,\n",
       " 'id_11': nan,\n",
       " 'id_12': 2.0,\n",
       " 'id_13': nan,\n",
       " 'id_14': nan,\n",
       " 'id_15': 3.0,\n",
       " 'id_17': nan,\n",
       " 'id_19': nan,\n",
       " 'id_20': nan,\n",
       " 'id_30': 86.0,\n",
       " 'id_31': 136.0,\n",
       " 'id_32': nan,\n",
       " 'id_33': 461.0,\n",
       " 'id_36': 2.0,\n",
       " 'id_37': 2.0,\n",
       " 'id_38': 2.0,\n",
       " 'DeviceType': 2.0,\n",
       " 'DeviceInfo': 2740.0,\n",
       " 'device_name': 13.0,\n",
       " 'device_version': 380.0,\n",
       " 'OS_id_30': 6.0,\n",
       " 'version_id_30': 49.0,\n",
       " 'browser_id_31': 31.0,\n",
       " 'version_id_31': 43.0,\n",
       " 'screen_width': 247.0,\n",
       " 'screen_height': 216.0,\n",
       " 'had_id': nan,\n",
       " 'TransactionAmt_to_mean_card1': 0.60791015625,\n",
       " 'TransactionAmt_to_std_card1': 0.5892407151753276,\n",
       " 'TransactionAmt_to_mean_card4': 0.443115234375,\n",
       " 'TransactionAmt_to_std_card4': 0.258544921875,\n",
       " 'TransactionAmt_to_mean_addr1': 0.44580078125,\n",
       " 'TransactionAmt_to_std_addr1': 0.2542401622950759,\n",
       " 'id_02_to_mean_card1': nan,\n",
       " 'id_02_to_std_card1': nan,\n",
       " 'id_02_to_mean_card4': nan,\n",
       " 'id_02_to_std_card4': nan,\n",
       " 'id_02_to_mean_addr1': nan,\n",
       " 'id_02_to_std_addr1': nan,\n",
       " 'D15_to_mean_card1': 2.51953125,\n",
       " 'D15_to_std_card1': 1.8510164167766447,\n",
       " 'D15_to_mean_card4': 1.8662109375,\n",
       " 'D15_to_std_card4': 1.541015625,\n",
       " 'D15_to_mean_addr1': 1.611328125,\n",
       " 'D15_to_std_addr1': 1.486472273547533,\n",
       " 'TransactionAmt_Log': 4.078125,\n",
       " 'TransactionAmt_decimal': 0.0,\n",
       " 'Transaction_day_of_week': 0.0,\n",
       " 'Transaction_hour': 0.0,\n",
       " 'id_02__id_20': 270749.0,\n",
       " 'id_02__D8': 259912.0,\n",
       " 'D11__DeviceInfo': 248.0,\n",
       " 'DeviceInfo__P_emaildomain': 8527.0,\n",
       " 'P_emaildomain__C2': 5141.0,\n",
       " 'card2__dist1': 23016.0,\n",
       " 'card1__card5': 12999.0,\n",
       " 'card2__id_20': 6795.0,\n",
       " 'card5__P_emaildomain': 750.0,\n",
       " 'addr1__card1': 35560.0,\n",
       " 'card1_count_full': 1794.0,\n",
       " 'card2_count_full': 70496.0,\n",
       " 'card3_count_full': 956845.0,\n",
       " 'card4_count_full': 719649.0,\n",
       " 'card5_count_full': 102930.0,\n",
       " 'card6_count_full': 824959.0,\n",
       " 'id_36_count_full': 819269.0,\n",
       " 'id_01_count_dist': 446307.0,\n",
       " 'id_31_count_dist': 450258.0,\n",
       " 'id_33_count_dist': 517251.0,\n",
       " 'id_36_count_dist': 449555.0,\n",
       " 'P_emaildomain_rank': 2580.0,\n",
       " 'P_emaildomain_traffic_rank': 904.0,\n",
       " 'P_emaildomain_sites': 5495.0,\n",
       " 'R_emaildomain_rank': nan,\n",
       " 'R_emaildomain_traffic_rank': nan,\n",
       " 'R_emaildomain_sites': nan,\n",
       " 'P_emaildomain_bin': 5.0,\n",
       " 'P_emaildomain_suffix': 0.0,\n",
       " 'R_emaildomain_bin': 6.0,\n",
       " 'R_emaildomain_suffix': 6.0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.loc[2987002].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tX_train, tX_test, ty_train, ty_test = train_test_split(X.reset_index(drop=True), \n",
    "#                                                         y,\n",
    "#                                                         train_size=0.8, test_size=0.20,\n",
    "#                                                         stratify=y)\n",
    "\n",
    "# tpot = TPOTClassifier(generations=6, population_size=25, \n",
    "#                       verbosity=2, \n",
    "#                       scoring='roc',\n",
    "#                       n_jobs=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tX_train = tX_train.reset_index(drop=True).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "# tX_test = tX_test.reset_index(drop=True).replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.any(np.isnan(tX_train)), np.all(np.isfinite(tX_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tpot.fit(tX_train, ty_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oskar/Notebooks/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 491,\n",
    "          'min_child_weight': 0.03454472573214212,\n",
    "          'feature_fraction': 0.3797454081646243,\n",
    "          'bagging_fraction': 0.4181193142567742,\n",
    "          'min_data_in_leaf': 106,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.006883242363721497,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'auc',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.3899927210061127,\n",
    "          'reg_lambda': 0.6485237330340494,\n",
    "          'random_state': 47,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds.\n",
      "[250]\ttraining's auc: 0.962531\tvalid_1's auc: 0.895617\n",
      "[500]\ttraining's auc: 0.984754\tvalid_1's auc: 0.908416\n",
      "[750]\ttraining's auc: 0.993985\tvalid_1's auc: 0.915462\n",
      "[1000]\ttraining's auc: 0.99775\tvalid_1's auc: 0.918845\n",
      "[1250]\ttraining's auc: 0.999173\tvalid_1's auc: 0.920399\n",
      "[1500]\ttraining's auc: 0.999675\tvalid_1's auc: 0.920805\n",
      "[1750]\ttraining's auc: 0.999871\tvalid_1's auc: 0.921073\n",
      "[2000]\ttraining's auc: 0.999956\tvalid_1's auc: 0.921459\n",
      "[2250]\ttraining's auc: 0.999985\tvalid_1's auc: 0.921878\n",
      "[2500]\ttraining's auc: 0.999995\tvalid_1's auc: 0.921819\n",
      "Early stopping, best iteration is:\n",
      "[2240]\ttraining's auc: 0.999984\tvalid_1's auc: 0.921911\n",
      "Fold 1 | AUC: 0.9219112539480251\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[250]\ttraining's auc: 0.961452\tvalid_1's auc: 0.912725\n",
      "[500]\ttraining's auc: 0.98508\tvalid_1's auc: 0.925987\n",
      "[750]\ttraining's auc: 0.994693\tvalid_1's auc: 0.933356\n",
      "[1000]\ttraining's auc: 0.998221\tvalid_1's auc: 0.936132\n",
      "[1250]\ttraining's auc: 0.999388\tvalid_1's auc: 0.937437\n",
      "[1500]\ttraining's auc: 0.999778\tvalid_1's auc: 0.937915\n",
      "[1750]\ttraining's auc: 0.999922\tvalid_1's auc: 0.938254\n",
      "[2000]\ttraining's auc: 0.999976\tvalid_1's auc: 0.93812\n",
      "[2250]\ttraining's auc: 0.999993\tvalid_1's auc: 0.937857\n",
      "Early stopping, best iteration is:\n",
      "[1827]\ttraining's auc: 0.999945\tvalid_1's auc: 0.938341\n",
      "Fold 2 | AUC: 0.9383415135365081\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[250]\ttraining's auc: 0.96301\tvalid_1's auc: 0.913816\n",
      "[500]\ttraining's auc: 0.985741\tvalid_1's auc: 0.92818\n",
      "[750]\ttraining's auc: 0.994712\tvalid_1's auc: 0.933503\n",
      "[1000]\ttraining's auc: 0.998207\tvalid_1's auc: 0.935202\n",
      "[1250]\ttraining's auc: 0.999383\tvalid_1's auc: 0.935415\n",
      "[1500]\ttraining's auc: 0.999785\tvalid_1's auc: 0.9351\n",
      "Early stopping, best iteration is:\n",
      "[1201]\ttraining's auc: 0.99924\tvalid_1's auc: 0.935503\n",
      "Fold 3 | AUC: 0.9355028561327456\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[250]\ttraining's auc: 0.960387\tvalid_1's auc: 0.929316\n",
      "[500]\ttraining's auc: 0.984948\tvalid_1's auc: 0.944269\n",
      "[750]\ttraining's auc: 0.994559\tvalid_1's auc: 0.950464\n",
      "[1000]\ttraining's auc: 0.998179\tvalid_1's auc: 0.952474\n",
      "[1250]\ttraining's auc: 0.999388\tvalid_1's auc: 0.953245\n",
      "[1500]\ttraining's auc: 0.999787\tvalid_1's auc: 0.953238\n",
      "[1750]\ttraining's auc: 0.999922\tvalid_1's auc: 0.953082\n",
      "[2000]\ttraining's auc: 0.999975\tvalid_1's auc: 0.95277\n",
      "Early stopping, best iteration is:\n",
      "[1555]\ttraining's auc: 0.999828\tvalid_1's auc: 0.953275\n",
      "Fold 4 | AUC: 0.953274731176943\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[250]\ttraining's auc: 0.961763\tvalid_1's auc: 0.906519\n",
      "[500]\ttraining's auc: 0.985016\tvalid_1's auc: 0.921418\n",
      "[750]\ttraining's auc: 0.994352\tvalid_1's auc: 0.928392\n",
      "[1000]\ttraining's auc: 0.998038\tvalid_1's auc: 0.930314\n",
      "[1250]\ttraining's auc: 0.999316\tvalid_1's auc: 0.93087\n",
      "[1500]\ttraining's auc: 0.999755\tvalid_1's auc: 0.930927\n",
      "[1750]\ttraining's auc: 0.999915\tvalid_1's auc: 0.930849\n",
      "Early stopping, best iteration is:\n",
      "[1483]\ttraining's auc: 0.999738\tvalid_1's auc: 0.931001\n",
      "Fold 5 | AUC: 0.9310005745813813\n",
      "\n",
      "Mean AUC = 0.9360061858751205\n",
      "Out of folds AUC = 0.9367027989738442\n"
     ]
    }
   ],
   "source": [
    "NFOLDS = 5\n",
    "folds = KFold(n_splits=NFOLDS)\n",
    "\n",
    "columns = X.columns\n",
    "splits = folds.split(X, y)\n",
    "y_preds = np.zeros(X_test.shape[0])\n",
    "y_oof = np.zeros(X.shape[0])\n",
    "score = 0\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = columns\n",
    "  \n",
    "for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    \n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dvalid = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], \n",
    "                    verbose_eval=250, \n",
    "                    early_stopping_rounds=500)\n",
    "    \n",
    "    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n",
    "    \n",
    "    y_pred_valid = clf.predict(X_valid)\n",
    "    y_oof[valid_index] = y_pred_valid\n",
    "    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n",
    "    \n",
    "    score += roc_auc_score(y_valid, y_pred_valid) / NFOLDS\n",
    "    y_preds += clf.predict(X_test) / NFOLDS\n",
    "    \n",
    "    del X_train, X_valid, y_train, y_valid\n",
    "    gc.collect()\n",
    "    \n",
    "print(f\"\\nMean AUC = {score}\")\n",
    "print(f\"Out of folds AUC = {roc_auc_score(y, y_oof)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['isFraud'] = y_preds\n",
    "sub.to_csv(\"submission-20190915-B.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score according to Kaggle: **0.9452**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step: to reach **0.9460** (0.008 better)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
